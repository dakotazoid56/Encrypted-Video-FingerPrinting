{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af1d757d",
   "metadata": {},
   "source": [
    "# Data Collection From Net Unicorn Into snl-server-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210ee050",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2cfe95b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df59b891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import os\n",
    "import dotenv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from netunicorn.client.remote import RemoteClient, RemoteClientException\n",
    "from netunicorn.base import Experiment, ExperimentStatus, Pipeline\n",
    "from netunicorn.library.tasks.capture.tcpdump import StartCapture, StopNamedCapture\n",
    "from netunicorn.library.tasks.upload.fileio import UploadToFileIO\n",
    "from netunicorn.library.tasks.upload.webdav import UploadToWebDav\n",
    "from netunicorn.library.tasks.basic import SleepTask\n",
    "from netunicorn.library.tasks.measurements.ookla_speedtest import SpeedTest\n",
    "from netunicorn.library.tasks.video_watchers.youtube_watcher import WatchYouTubeVideo\n",
    "from netunicorn.library.tasks.video_watchers.vimeo_watcher import WatchVimeoVideo\n",
    "from netunicorn.library.tasks.video_watchers.twitch_watcher import WatchTwitchStream\n",
    "from netunicorn.base import DockerImage\n",
    "\n",
    "from collections import defaultdict\n",
    "from scapy.layers.inet import IP, TCP\n",
    "from scapy.all import rdpcap\n",
    "\n",
    "from math import ceil\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17de7ad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Private.env file used to store private variables such as filepaths and Ip address\n",
    "dotenv.load_dotenv('Private.env')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0acb1bf",
   "metadata": {},
   "source": [
    "## Capture Videos Reference (Created in DataCollection.ipynb)\n",
    "\n",
    "- For reference use only here\n",
    "- All Captures are Stored in folders labeled capture_#, defined in the next cell (on snl server)\n",
    "- Descriptions of each capture we took with videos, # nodes, # seconds, # loops, run #, and node name (if applicable)\n",
    "- Stored in snl-server-5.cs.ucsb.edu:/mnt/md0/cs190n/team_dn/capture_<#>/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f7246a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#capture_0 10 node\n",
    "video_links = {\n",
    "    'video0': 'https://www.youtube.com/watch?v=_KOpmohOznIiU',\n",
    "    'video1': 'https://www.youtube.com/watch?v=BGqfVOc_Rl4',\n",
    "    'video2': 'https://www.youtube.com/watch?v=XVO9CS8D4hQ',\n",
    "    'video3': 'https://www.youtube.com/watch?v=sYc1MsexWKE',\n",
    "    'video4': 'https://www.youtube.com/watch?v=KQwPyhcidUE',\n",
    "    'video5': 'https://www.youtube.com/watch?v=_BPexIx58Zg',\n",
    "    'video6': 'https://www.youtube.com/watch?v=MbHIxbbyOi8',\n",
    "    'video7': 'https://www.youtube.com/watch?v=gKCScoG83SA',\n",
    "    'video8': 'https://www.youtube.com/watch?v=w9iFuKzfoIs',\n",
    "    'video9': 'https://www.youtube.com/watch?v=slwVFwPjUx4',\n",
    "}\n",
    "\n",
    "#capture_1 15 nodes\n",
    "video_links = {\n",
    "    'video0': 'https://www.youtube.com/watch?v=vkfTSlTnaBQ',\n",
    "    'video1': 'https://www.youtube.com/watch?v=KT1-JQTiZGc',\n",
    "    'video2': 'https://www.youtube.com/watch?v=_38JDGnr0vA',\n",
    "}\n",
    "\n",
    "\n",
    "#capture_2 50 nodes videos >5 min(Did not get all videos/nodes)\n",
    "video_links = {\n",
    "    'video0': 'https://www.youtube.com/watch?v=vkfTSlTnaBQ',\n",
    "    'video1': 'https://www.youtube.com/watch?v=KT1-JQTiZGc',\n",
    "    'video2': 'https://www.youtube.com/watch?v=_38JDGnr0vA',\n",
    "    'video3': 'https://www.youtube.com/watch?v=GqulwE_yKww',\n",
    "    'video4': 'https://www.youtube.com/watch?v=v2WCnF3SSUE',\n",
    "    'video5': 'https://www.youtube.com/watch?v=hif5eI5pBxo',\n",
    "    'video6': 'https://www.youtube.com/watch?v=0e4qRdlfJcs',\n",
    "    'video7': 'https://www.youtube.com/watch?v=aIovmgzyuL0',\n",
    "    'video8': 'https://www.youtube.com/watch?v=k7cGyYaxUnI',\n",
    "    'video9': 'https://www.youtube.com/watch?v=4GL-X4LqfVc',\n",
    "}\n",
    "\n",
    "\n",
    "#capture_3 (Redoing 2 but with 40 nodes) videos > 5 min\n",
    "video_links = {\n",
    "    'video0': 'https://www.youtube.com/watch?v=vkfTSlTnaBQ',\n",
    "    'video1': 'https://www.youtube.com/watch?v=KT1-JQTiZGc',\n",
    "    'video2': 'https://www.youtube.com/watch?v=_38JDGnr0vA',\n",
    "    'video3': 'https://www.youtube.com/watch?v=GqulwE_yKww',\n",
    "    'video4': 'https://www.youtube.com/watch?v=v2WCnF3SSUE',\n",
    "    'video5': 'https://www.youtube.com/watch?v=hif5eI5pBxo',\n",
    "    'video6': 'https://www.youtube.com/watch?v=0e4qRdlfJcs',\n",
    "    'video7': 'https://www.youtube.com/watch?v=aIovmgzyuL0',\n",
    "    'video8': 'https://www.youtube.com/watch?v=k7cGyYaxUnI',\n",
    "    'video9': 'https://www.youtube.com/watch?v=4GL-X4LqfVc',\n",
    "}\n",
    "\n",
    "#capture_4 switching to vimeo 60 seconds, 20 nodes\n",
    "#  second run vimeo 60 seconds, 30 nodes\n",
    "#    TOTAL capture_4 = 50 nodes, 60 seconds\n",
    "\n",
    "video_links = {\n",
    "    'video0': 'https://vimeo.com/872245830',\n",
    "    'video1': 'https://vimeo.com/249226357',\n",
    "    'video2': 'https://vimeo.com/250383662',\n",
    "    'video3': 'https://vimeo.com/255370388',\n",
    "    'video4': 'https://vimeo.com/259411563',\n",
    " \n",
    "}\n",
    "\n",
    "#capture_5 Vimeo 60 seconds, 6 nodes, to determine preprocessing steps\n",
    "video_links = {\n",
    "    'video0': 'https://vimeo.com/872245830',\n",
    "    'video1': 'https://vimeo.com/249226357',\n",
    "}\n",
    "\n",
    "#capture_6 Vimeo 60 seconds,  (oops)\n",
    "# raspi-e4:5f:01:a0:34:a8\n",
    "video_links = {\n",
    "    'video0': 'https://vimeo.com/872245830',\n",
    "    'video1': 'https://vimeo.com/249226357',\n",
    "}\n",
    "\n",
    "\n",
    "#capture_7 Vimeo 60 seconds, 1 nodes, looped 25 times (NIK)\n",
    "video_links = {\n",
    "    'video0': 'https://vimeo.com/872245830',\n",
    "    'video1': 'https://vimeo.com/249226357',\n",
    "    'video2': 'https://vimeo.com/250383662',\n",
    "    'video3': 'https://vimeo.com/255370388',\n",
    "    'video4': 'https://vimeo.com/259411563',\n",
    "}\n",
    "\n",
    "#capture_8 Vimeo 60 seconds, 1 nodes, looped 50 times (Dakota) from 'raspi-e4:5f:01:ac:e3:80' node\n",
    "video_links = {\n",
    "    'video0': 'https://vimeo.com/872245830',\n",
    "    'video1': 'https://vimeo.com/249226357',\n",
    "    'video2': 'https://vimeo.com/250383662',\n",
    "    'video3': 'https://vimeo.com/255370388',\n",
    "    'video4': 'https://vimeo.com/259411563',\n",
    "}\n",
    "\"\"\"\n",
    "#capture_9 DEMONSTRATION CAPTURE , Vimeo 60 seconds, 5 nodes, 1 loops (total 50 .pcap files)\n",
    "# Second watch Vimeo 60 seconds, 2 nodes, 2 loops (total 20 .pcap files)\n",
    "\n",
    "video_links = {\n",
    "    'video0': 'https://vimeo.com/872245830',\n",
    "    'video1': 'https://vimeo.com/249226357',\n",
    "    'video2': 'https://vimeo.com/250383662',\n",
    "    'video3': 'https://vimeo.com/255370388',\n",
    "    'video4': 'https://vimeo.com/259411563',\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96c4254",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e92096d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to recursively search through snl directory and find all pcap files and return them in list\n",
    "# Example input = '/mnt/md0/cs190n/team_dn/capture_0'\n",
    "def get_all_pcap_files(base_input_directory):\n",
    "    all_pcap_files = []\n",
    "    for root, dirs, files in os.walk(base_input_directory):\n",
    "        for file_name in files:\n",
    "            if file_name.endswith(\".pcap\"):\n",
    "                full_path = os.path.join(root, file_name)\n",
    "                all_pcap_files.append(full_path)\n",
    "                \n",
    "    return all_pcap_files\n",
    "\n",
    "\n",
    "# Takes input of pcap file, then returns the label (Assuming the label is in first part of word) \n",
    "#    - Correct Labeling structure is defined in pipeline in DataCollection by variables so should be uniform\n",
    "# Example input = /mnt/md0/cs190n/team_dn/capture_0/27f89405-f1ca-4642-991b-0f71f8e0faab/tmp/video0.pcap\n",
    "# will return video0, to be used as a label\n",
    "\n",
    "# input video0_1.pcap , or just video0.pcap\n",
    "# output video0\n",
    "\n",
    "def extract_video_name(pcap_path):\n",
    "    base_name = os.path.basename(pcap_path)\n",
    "    possible_name = base_name.split('.')[0]\n",
    "    if '_' in possible_name:\n",
    "        name = base_name.split('_')[0]\n",
    "    else:\n",
    "        name = possible_name\n",
    "    \n",
    "    return name\n",
    "\n",
    "\n",
    "# function that will take in a pcap stream and place into a dataframe with columns for up_bps, down_bps ....\n",
    "# the rows will be each time interval(index) defined in interval_float\n",
    "# also returns the label for the dataFrame\n",
    "def pcap_to_df(pcap_path, src_ip, interval_float=0.25):\n",
    "    packets = rdpcap(pcap_path)\n",
    "     # Check if the pcap file is empty\n",
    "    if not packets:\n",
    "        print(f\"No packets found in {pcap_path}. Returning an empty DataFrame.\")\n",
    "        return pd.DataFrame(), None\n",
    "    \n",
    "    interval = str(f\"{interval_float}S\")\n",
    "    # Initialize data structures\n",
    "    \n",
    "    data = defaultdict(lambda: {'up_bps': 0, 'down_bps': 0, 'up_pps': 0, 'down_pps': 0, 'up_plen': 0, 'down_plen': 0})\n",
    "    start_time = packets[0].time\n",
    "\n",
    "    # Process packets\n",
    "    for packet in packets:\n",
    "        if IP in packet and TCP in packet:\n",
    "            # Calculate time offset\n",
    "            current_time = packet.time\n",
    "            chunk_key = int((current_time - start_time) // interval_float)\n",
    "            \n",
    "            # Determine direction and accumulate data\n",
    "            direction = 'up' if packet[IP].src == src_ip else 'down'\n",
    "            data[chunk_key][f'{direction}_bps'] += len(packet)\n",
    "            data[chunk_key][f'{direction}_pps'] += 1\n",
    "            data[chunk_key][f'{direction}_plen'] += len(packet)\n",
    "\n",
    "    if not data:\n",
    "        print(f\"No relevant packet data found in {pcap_path}.\")\n",
    "        return pd.DataFrame(), None       \n",
    "    \n",
    "    last_chunk = max(data.keys())\n",
    "    first_chunk = min(data.keys())\n",
    "\n",
    "    # Ensure every interval from the first actual data chunk to the last has an entry\n",
    "    for time_chunk in range(first_chunk, last_chunk + 1):\n",
    "        if time_chunk not in data:\n",
    "            data[time_chunk] = {'up_bps': 0, 'down_bps': 0, 'up_pps': 0, 'down_pps': 0, 'up_plen': 0, 'down_plen': 0}\n",
    "       \n",
    "    df_data = []\n",
    "    \n",
    "    #Relabel DataFrame to Start at 0 and increment in interval\n",
    "    time_offset = first_chunk * interval_float\n",
    "    for time_chunk in sorted(data.keys()):\n",
    "        chunk_data = data[time_chunk]\n",
    "        chunk_data['time'] = (time_chunk * interval_float) - time_offset\n",
    "        df_data.append(chunk_data)\n",
    "\n",
    "    df = pd.DataFrame(df_data)\n",
    "    df.set_index('time', inplace=True)\n",
    "    \n",
    "    row_label = extract_video_name(pcap_path)\n",
    "    \n",
    "    return df, row_label\n",
    "\n",
    "#function takes in the root_directory of where all the .pcap files are located, and converts each video into a seperate dataframe\n",
    "#return the list of dataframes, and their corresponding video label\n",
    "#the cur_ip is used to determine the which packet flow to watch, b_interval is the burst_interval, and t_capture is the capture time\n",
    "#the function also has the option to print_pcap which is to print Process_Pcap_Count after every pcap, and num_pcaps to determine if you want a certain amount\n",
    "def pcap_to_df_list(root_dir, cur_ip, b_interval, t_capture, print_pcap=True, num_pcaps=-1):\n",
    "    # Get all PCAP files in the directory specified in the pipeline, or chosen\n",
    "    all_pcap_files = get_all_pcap_files(root_dir)\n",
    "    print(f\"Obtained {len(all_pcap_files)} .pcap Files\")  \n",
    "    pcap_files = all_pcap_files if num_pcaps < 0 else all_pcap_files[:num_pcaps]\n",
    "\n",
    "    # Lists which are populated by pcap_to_df function\n",
    "    raw_video_dfs = []\n",
    "    labels = []\n",
    "    \n",
    "    #Iterate through each PCAP and turn it into df, add to raw_videos_dfs and label to labels\n",
    "    for count, pcap_file in enumerate(pcap_files):\n",
    "        cur_df, cur_label = pcap_to_df(pcap_file, cur_ip, b_interval)\n",
    "        raw_video_dfs.append(cur_df)\n",
    "        labels.append(cur_label)\n",
    "        count += 1\n",
    "        if print_pcap:\n",
    "            print(f\"Process PCAP #{count}\")\n",
    "    print(f\"Finished Processing {len(raw_video_dfs)} pcaps from {root_dir}\")\n",
    "        \n",
    "    return raw_video_dfs, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb01610b",
   "metadata": {},
   "source": [
    "## Turning .pcaps into DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3574e2",
   "metadata": {},
   "source": [
    "- First recursively obtains all all the .pcaps in the root_dir (obtained through netUnicorn DataCollection.ipynb placed on snl-server-5)\n",
    "- Determine your constants. \n",
    "- burst_interval: For the scklearn classifier used 5 seconds, pyTorch CNN we used .25 seconds\n",
    "- Each Video is Turned into 1 entire Dataframe\n",
    "- List of Dataframes is stored in raw_video_dfs and with their labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afe303f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtained 250 .pcap Files\n",
      "No packets found in /mnt/md0/cs190n/team_dn/capture_8/3b9d009b-b6f1-411f-9d55-f0ae184c7f58/tmp/video3_1_7.pcap. Returning an empty DataFrame.\n",
      "No packets found in /mnt/md0/cs190n/team_dn/capture_8/3b9d009b-b6f1-411f-9d55-f0ae184c7f58/tmp/video4_1_18.pcap. Returning an empty DataFrame.\n",
      "No packets found in /mnt/md0/cs190n/team_dn/capture_8/3b9d009b-b6f1-411f-9d55-f0ae184c7f58/tmp/video1_1_18.pcap. Returning an empty DataFrame.\n",
      "No packets found in /mnt/md0/cs190n/team_dn/capture_8/3b9d009b-b6f1-411f-9d55-f0ae184c7f58/tmp/video1_1_49.pcap. Returning an empty DataFrame.\n",
      "No packets found in /mnt/md0/cs190n/team_dn/capture_8/3b9d009b-b6f1-411f-9d55-f0ae184c7f58/tmp/video2_1_40.pcap. Returning an empty DataFrame.\n",
      "No packets found in /mnt/md0/cs190n/team_dn/capture_8/3b9d009b-b6f1-411f-9d55-f0ae184c7f58/tmp/video3_1_45.pcap. Returning an empty DataFrame.\n",
      "No packets found in /mnt/md0/cs190n/team_dn/capture_8/3b9d009b-b6f1-411f-9d55-f0ae184c7f58/tmp/video3_1_47.pcap. Returning an empty DataFrame.\n",
      "No packets found in /mnt/md0/cs190n/team_dn/capture_8/3b9d009b-b6f1-411f-9d55-f0ae184c7f58/tmp/video2_1_47.pcap. Returning an empty DataFrame.\n",
      "No packets found in /mnt/md0/cs190n/team_dn/capture_8/3b9d009b-b6f1-411f-9d55-f0ae184c7f58/tmp/video2_1_46.pcap. Returning an empty DataFrame.\n",
      "No packets found in /mnt/md0/cs190n/team_dn/capture_8/3b9d009b-b6f1-411f-9d55-f0ae184c7f58/tmp/video2_1_21.pcap. Returning an empty DataFrame.\n",
      "No relevant packet data found in /mnt/md0/cs190n/team_dn/capture_8/3b9d009b-b6f1-411f-9d55-f0ae184c7f58/tmp/video3_1_41.pcap.\n",
      "No packets found in /mnt/md0/cs190n/team_dn/capture_8/3b9d009b-b6f1-411f-9d55-f0ae184c7f58/tmp/video3_1_33.pcap. Returning an empty DataFrame.\n"
     ]
    }
   ],
   "source": [
    "#Constants Used for the Experiment\n",
    "current_ip = os.getenv('NODE_IP')                             # Determine which flow to take packets from\n",
    "burst_interval = .25                                          # Use to aggregate data into intervals (5s for concat Sklearn, .25 for )\n",
    "capture_time = 60                                             # Time to crop video packets\n",
    "intervals_per_row = int(capture_time / burst_interval)        # 30 seconds / 0.25 seconds per interval\n",
    "one_node_dir = f\"{os.getenv('SNL_SERVER_PATH')}capture_8\"\n",
    "multi_node_dir = f\"{os.getenv('SNL_SERVER_PATH')}capture_4\"\n",
    "        \n",
    "raw_one_node_dfs, one_node_labels = pcap_to_df_list(one_node_dir, current_ip, burst_interval, capture_time, False, -1)\n",
    "raw_multi_node_dfs, multi_node_labels = pcap_to_df_list(multi_node_dir, current_ip, burst_interval, capture_time, False, -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d59883",
   "metadata": {},
   "source": [
    "## Normalize Video DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade76b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crop all dataframes(1 video each) to same length and remove any short captures\n",
    "def normalize_videos(raw_video_dfs_list, labels, crop_interval):\n",
    "    video_dfs = []\n",
    "    for df, label in zip(raw_video_dfs_list, labels):\n",
    "        # Truncate or pad the DataFrame\n",
    "        if len(df) > crop_interval:\n",
    "            df = df.iloc[:crop_interval].copy()\n",
    "            df['video'] = label\n",
    "            video_dfs.append(df)\n",
    "        else:\n",
    "            print(\"Packet To Short, Removing from DataFrame List\")\n",
    "\n",
    "    print(\"Created Videos DataFrame\")        \n",
    "    print(f\"Total Successful DFs: {len(video_dfs)}\")   \n",
    "    \n",
    "    return video_dfs\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7e8a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_node_dfs = normalize_videos(raw_one_node_dfs, one_node_labels, intervals_per_row)\n",
    "multi_node_dfs = normalize_videos(raw_multi_node_dfs, multi_node_labels, intervals_per_row)\n",
    "\n",
    "\n",
    "# Create a concated dataframe for the Sklearn model (2D array) by combining all video_dfs(3D)\n",
    "one_node_concat = pd.concat(one_node_dfs, ignore_index=False)\n",
    "multi_node_concat = pd.concat(multi_node_dfs, ignore_index=False)\n",
    "\n",
    "#TIME SAVER: AFTER RUN, SAVE CONCAT DF TO CSV TO REFERENCE LATER\n",
    "#one_node_concat.to_csv(f'{one_node_dir}_{capture_time}_{burst_interval}_.csv')\n",
    "#multi_node_concat.to_csv(f'{multi_node_dir}_{capture_time}_{burst_interval}_.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42dff84",
   "metadata": {},
   "source": [
    "## View Video DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ad5f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphing Functions to Help Visualize Data Collection\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "def plot_multiple_streams(df, num_rows):\n",
    "    \"\"\"\n",
    "    Plots multiple time series (rows) from the DataFrame on the same graph as line plots.\n",
    "\n",
    "    :param df: DataFrame containing the data.\n",
    "    :param num_rows: Number of rows (time series) to plot.\n",
    "    \"\"\"\n",
    "    num_rows = min(num_rows, len(df))\n",
    "    color_cycle = itertools.cycle(plt.rcParams['axes.prop_cycle'].by_key()['color'])\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot each row as a separate line plot\n",
    "    for i in range(num_rows):\n",
    "        plt.plot(df.columns, df.iloc[i], linestyle='-', marker='o', color=next(color_cycle), label=f'Video {i+1}')\n",
    "\n",
    "    plt.title('PPS Flow of Multiple YouTube Video Streams')\n",
    "    plt.xlabel('Time Intervals')\n",
    "    plt.ylabel('Average PPS')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()\n",
    "    #plt.grid(True)\n",
    "    #plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_time_series(df, column, title='Time Series Plot', ylabel=None):\n",
    "    \"\"\"\n",
    "    Plots a time series from a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The resampled DataFrame.\n",
    "    column (str): The name of the column to plot.\n",
    "    title (str, optional): The title of the plot. Defaults to 'Time Series Plot'.\n",
    "    ylabel (str, optional): The label for the y-axis. Defaults to the same as the column name.\n",
    "    \"\"\"\n",
    "\n",
    "    if ylabel is None:\n",
    "        ylabel = column\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(df.index, df[column], label=column)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_time_series_dfs(dataframes, column_label, feature_label, title_string):\n",
    "    \"\"\"\n",
    "    Plot line graphs for a specific column from a list of dataframes on the same plot.\n",
    "\n",
    "    Parameters:\n",
    "    dataframes (list of pd.DataFrame): List of dataframes to plot.\n",
    "    column_label (str): The column label to plot from each dataframe.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    for df in dataframes:\n",
    "        if column_label in df.columns:\n",
    "            plt.plot(df.index, df[column_label], label=df[column_label].name)\n",
    "        else:\n",
    "            print(f\"Column '{column_label}' not found in one of the dataframes.\")\n",
    "\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel(column_label)\n",
    "    plt.title(f'Time Series Plot for {feature_label} {column_label} on {title_string}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_all_time_series_dfs(raw_df_list,df_list, labels, feature):\n",
    "    unique_labels = sorted(set(labels))\n",
    "    for label in unique_labels:\n",
    "        feature_raw_dfs = [df for df, l in zip(raw_df_list, labels) if l == label]\n",
    "        plot_time_series_dfs(feature_raw_dfs, feature, label, 'Raw Data')\n",
    "\n",
    "        feature_processed_dfs = [df for df, l in zip(df_list, labels) if l == label]\n",
    "        plot_time_series_dfs(feature_processed_dfs, feature, label, 'Cropped Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b553bacd",
   "metadata": {},
   "source": [
    "### Graph Structures\n",
    "- View Every Capture Stream Per Video with selected feature, all in one graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad73f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_time_series_dfs(raw_one_node_dfs, one_node_dfs, one_node_labels ,'down_bps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4ec735",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_time_series_dfs(raw_multi_node_dfs, multi_node_dfs, multi_node_labels ,'down_bps')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d954f714",
   "metadata": {},
   "source": [
    "# Classifier Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29562e1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%pip install scikit-learn\n",
    "%pip install tensorflow\n",
    "%pip install trustee\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import plot_tree\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from trustee import ClassificationTrustee\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e90b74",
   "metadata": {},
   "source": [
    "## Sklearn RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf73510",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to train a Random Forest Classifier on a Dataframe\n",
    "#Has the option to print_report from sklearn, or print_trustee_report which is to print the trustee report \n",
    "def Random_Forest_Classifier(df,print_report=False, print_trustee_report=False):\n",
    "\n",
    "    X = df.drop('video', axis=1)  # Features from DataFrame\n",
    "    y = df['video']  # Labels from your existing array\n",
    "\n",
    "    # Split the dataset into training and testing sets (70% train, 30% test)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Create and train the Random Forest Classifier\n",
    "    clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy or other performance metric\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    if print_report:\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        \n",
    "    if print_trustee_report:\n",
    "        trustee = ClassificationTrustee(expert=clf)\n",
    "        trustee.fit(X_train, y_train, num_samples=len(X_train) // 2, num_iter=20, train_size=0.99)\n",
    "\n",
    "        # Display Trustee Results\n",
    "        _, dt, _, score = trustee.explain()\n",
    "        print(f\"Training score of pruned DT: {score}\")\n",
    "        dt_y_pred = dt.predict(X_train)\n",
    "        print(\"Model explanation global fidelity report:\")\n",
    "        print(metrics.classification_report(clf.predict(X_train), dt_y_pred))\n",
    "        print(\"Model explanation score report:\")\n",
    "        print(metrics.classification_report(y_train, dt_y_pred))\n",
    "\n",
    "        # plot a tree\n",
    "        fig = plt.figure(figsize=(25,20))\n",
    "        plot_tree(dt, feature_names=X_train.columns, class_names=sorted(df['video'].unique()), filled=True, max_depth=5)\n",
    "\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "def Random_Forest_Classifier_With_Interval_Report(videos_concat):\n",
    "    print(\"COMBINED TIME FRAME ACCURACY REPORT\")\n",
    "\n",
    "    #Print The Report for the OverAll Concat Dataframe accuracy (average of all timeframe accuracies)\n",
    "    Random_Forest_Classifier(videos_concat, print_report=True)\n",
    "\n",
    "    #Print all the interval accuracy's in the videos_concat, then keep track of the best one to use in Trustee\n",
    "    best_accuracy = 0\n",
    "    best_df = None\n",
    "    best_time = None\n",
    "    print(\"INDIVIDUAL TIME FRAME ACCURACY REPORT\")\n",
    "    unique_times = videos_concat['time'].unique()\n",
    "    for time in unique_times:\n",
    "        df = videos_concat[videos_concat['time'] == time].reset_index(drop=True)\n",
    "        accuracy = Random_Forest_Classifier(df)\n",
    "        print(f\"Time: {time}, Accuracy: {accuracy}\")\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_df = df\n",
    "            best_time = time\n",
    "\n",
    "    print()        \n",
    "    print(f\"Best Time: {best_time}, Best Accuracy: {best_accuracy}\")\n",
    "    \n",
    "    return best_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a34da0",
   "metadata": {},
   "source": [
    "### View Overall Results for Concated Dataframe, as well as the individual accuracy per time frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d02458",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TIME SAVER: Obtain dataframe from CSV so no need to preprocess data every use (Keep Commented Unless Used)\n",
    "\n",
    "one_node_csv_name = 'capture_8_5s_concat2.csv'       # Looking At One Node Classifier       (5 vidoes, 50 watcher per video)\n",
    "\n",
    "one_node_concat = pd.read_csv(f\"{os.getenv('SNL_SERVER_PATH')}{one_node_csv_name}\", index_col=None)\n",
    "\n",
    "one_node_best_df = Random_Forest_Classifier_With_Interval_Report(one_node_concat)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2c7a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TIME SAVER: Obtain dataframe from CSV so no need to preprocess data every use (Keep Commented Unless Used)\n",
    "\n",
    "multi_node_csv_name = 'capture_4_5s_concat2.csv'       # Looking At One Node Classifier       (5 vidoes, 50 watcher per video)\n",
    "\n",
    "multi_node_concat = pd.read_csv(f\"{os.getenv('SNL_SERVER_PATH')}{multi_node_csv_name}\", index_col=None)\n",
    "\n",
    "multi_node_best_df = Random_Forest_Classifier_With_Interval_Report(multi_node_concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7f75ef",
   "metadata": {},
   "source": [
    "## Trustee Model Evaluation RandomForestClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668ee7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the Trustee Visualization Report on the Best Preforming time frame to visualize the choices that are being made\n",
    "Random_Forest_Classifier(one_node_best_df, print_trustee_report=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff10ae2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the Trustee Visualization Report on the Best Preforming time frame to visualize the choices that are being made\n",
    "Random_Forest_Classifier(multi_node_best_df, print_trustee_report=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a8366f",
   "metadata": {},
   "source": [
    "## PyTorch Convolutional Neural Network\n",
    "- Take in video_dfs which is a list of dataFrames, where each dataframe is one video\n",
    "- Is able to view connections between timeframes, which the RandomForestClassifier can not do (3D vs 2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140560e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN uses the video_dfs that are created in the Normalize Video DataFrames Cells\n",
    "# DATA IS NOT STORED ON SNL-SERVER, HAS TO BE PROCESSED TO BE RUN\n",
    "# EXAMPLE WITH EXAMPLE DATASET (Only 25 videos)\n",
    "def Convolutional_Neural_Network(list_dfs):\n",
    "    \n",
    "    # Prepare the data\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "\n",
    "    for df in list_dfs:\n",
    "        # Assuming the label is in the last column\n",
    "        X_list.append(df.iloc[:, :-1].values)  # Features\n",
    "        y_list.append(df.iloc[0, -1])  # Label (assuming all rows in a DF have the same label)\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    X = np.array(X_list)\n",
    "    y = np.array(y_list)\n",
    "\n",
    "    # Reshape X for CNN input (samples, time segments, features, 1)\n",
    "    X = X.reshape((X.shape[0], X.shape[1], X.shape[2], 1))\n",
    "\n",
    "    # Encode the labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    y_categorical = to_categorical(y_encoded)\n",
    "\n",
    "    # Split the dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_categorical, test_size=0.3, random_state=45)\n",
    "\n",
    "    # Create the CNN model\n",
    "    model2 = Sequential()\n",
    "    model2.add(Conv2D(32, (3, 3), activation='relu', input_shape=(X.shape[1], X.shape[2], 1)))\n",
    "    model2.add(MaxPooling2D((2, 2)))\n",
    "    model2.add(Flatten())\n",
    "    model2.add(Dense(64, activation='relu'))\n",
    "    model2.add(Dense(y_categorical.shape[1], activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    model2.fit(X_train, y_train, epochs=30, batch_size=32)\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss, accuracy = model2.evaluate(X_test, y_test)\n",
    "    print(f\"Test Accuracy: {accuracy*100}%\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694994f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to be removed\n",
    "#rows_to_remove = [0, 5, 10]\n",
    "\n",
    "# Iterate over each DataFrame and drop the specified columns\n",
    "#for df in one_node_dfs:\n",
    "#    df.drop(rows_to_remove, axis=0, inplace=True, errors='ignore')\n",
    "\n",
    "#Convolutional_Neural_Network(one_node_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91395ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Columns to be removed\n",
    "#rows_to_remove = [0, 5, 10]\n",
    "\n",
    "# Iterate over each DataFrame and drop the specified columns\n",
    "#for df in one_node_dfs:\n",
    "#    df.drop(rows_to_remove, axis=0, inplace=True, errors='ignore')\n",
    "\n",
    "Convolutional_Neural_Network(multi_node_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c33efa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### BEEEEEEEEEEP (To know when finished)\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "def play_beep():\n",
    "    framerate = 44100\n",
    "    duration = 1  # in seconds\n",
    "    frequency = 500  # Hz, standard A4 tone\n",
    "\n",
    "    t = np.linspace(0, duration, int(framerate * duration))\n",
    "    audio_data = np.sin(2 * np.pi * frequency * t)\n",
    "\n",
    "    display(Audio(audio_data, rate=framerate, autoplay=True))\n",
    "\n",
    "# Play beep when the code finishes\n",
    "play_beep()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306fa9fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
